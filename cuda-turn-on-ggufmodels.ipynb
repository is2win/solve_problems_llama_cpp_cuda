{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Решение проблемы: Не цепляет CUDA Llama_cpp в Kaggle\n\nВозможно есть варианты проще, но этот вариант помог задействовать GPU с Kaggle","metadata":{}},{"cell_type":"markdown","source":"### Проверка драйвера\n\nНачинаем с проверки версии драйвера <br>\nНа момент написания вижу такое сообщение: <br>Build cuda_12.3.r12.3/compiler.33567101_0<br>Считаю что у меня версия 12.3","metadata":{}},{"cell_type":"code","source":"!nvcc --version","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T07:42:51.213905Z","iopub.execute_input":"2024-08-30T07:42:51.214208Z","iopub.status.idle":"2024-08-30T07:42:52.224318Z","shell.execute_reply.started":"2024-08-30T07:42:51.214174Z","shell.execute_reply":"2024-08-30T07:42:52.223371Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Выбор команды установки\nНа гитхабе данной библиотеки (https://github.com/abetlen/llama-cpp-python) есть описания разных способов установки<br>Нам подходит вариант с выбором **Pre-built Wheel (New)**<br>\n\n----------\n**Шаблон команды**: !pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/<cuda-version>\n----------\n    \nВместо **\\<cuda-version\\>** нужно подставить один из вариантов ниже:\n\ncu121: CUDA 12.1<br>\ncu122: CUDA 12.2<br>\ncu123: CUDA 12.3<br>\ncu124: CUDA 12.4<br>\n    \n**В моем случае - 12.3**\n    \n!pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123","metadata":{}},{"cell_type":"code","source":"!pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:47:36.811073Z","iopub.execute_input":"2024-08-30T07:47:36.811520Z","iopub.status.idle":"2024-08-30T07:48:34.606402Z","shell.execute_reply.started":"2024-08-30T07:47:36.811475Z","shell.execute_reply":"2024-08-30T07:48:34.605246Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu123\nCollecting llama-cpp-python\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu123/llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl (444.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.5/444.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Установка доп библиотек - опционально\nДалее проверим как работает данный метод и проведу установку дополнительных библиотек langchain","metadata":{}},{"cell_type":"code","source":"!pip install langchain langchain_community","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:48:56.504807Z","iopub.execute_input":"2024-08-30T07:48:56.505608Z","iopub.status.idle":"2024-08-30T07:49:14.529293Z","shell.execute_reply.started":"2024-08-30T07:48:56.505558Z","shell.execute_reply":"2024-08-30T07:49:14.528196Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.2.14-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.35 (from langchain)\n  Downloading langchain_core-0.2.36-py3-none-any.whl.metadata (6.2 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.107-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.35->langchain)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (4.12.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.2.14-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.36-py3-none-any.whl (395 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.107-py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.2 requires cubinlinker, which is not installed.\ncudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.15 langchain-core-0.2.36 langchain-text-splitters-0.2.2 langchain_community-0.2.14 langsmith-0.1.107 packaging-24.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Блок импортов для тестирования","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain_community.llms import LlamaCpp\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\nfrom langchain_core.prompts import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:49:24.937912Z","iopub.execute_input":"2024-08-30T07:49:24.938798Z","iopub.status.idle":"2024-08-30T07:49:44.196595Z","shell.execute_reply.started":"2024-08-30T07:49:24.938753Z","shell.execute_reply":"2024-08-30T07:49:44.195598Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Делаем загрузку готовой модели с HF","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import hf_hub_download\n\n# Указываю имя репозитория и название скачиваемой модели\nmodel_name = \"lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF\"\nmodel_file = \"Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n\n\n# Загрузка с Hugging Face Hub\nmodel_path = hf_hub_download(\n    model_name,\n    filename=model_file,\n    local_dir='models/',  # Загрузку сделаем в папку \"models\" - опционально\n    token=\"token\"  #тут указываем ваш токен доступа с huggingface (Setting -> Access Toekns -> New token -> Generate Token)\n)\n\nprint(\"My model path:\", model_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:51:43.868153Z","iopub.execute_input":"2024-08-30T07:51:43.869461Z","iopub.status.idle":"2024-08-30T07:55:56.937040Z","shell.execute_reply.started":"2024-08-30T07:51:43.869398Z","shell.execute_reply":"2024-08-30T07:55:56.936217Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Mistral-Nemo-Instruct-2407-Q4_K_M.gguf:   0%|          | 0.00/7.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eae706e37a84a5a9dc731e177930031"}},"metadata":{}},{"name":"stdout","text":"My model path: models/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Инициализация модели\n\nПосле инициализации модели вижу, что все две карты подцепились\n\n-----\n\nllm_load_tensors: offloaded 41/41 layers to GPU<br>\nllm_load_tensors:        CPU buffer size =   360.00 MiB<br>\nllm_load_tensors:      CUDA0 buffer size =  3265.43 MiB<br>\nllm_load_tensors:      CUDA1 buffer size =  3497.87 MiB<br>\n\n-----\n\nBLAS = 1<br>","metadata":{}},{"cell_type":"code","source":"# Путь до модели\nmodel_path = \"/kaggle/working/models/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n\n# Инициализирую модель через LlamaCpp\nllm = LlamaCpp(\n    model_path=model_path,\n    temperature=0.75,\n    max_tokens=200,\n    n_gpu_layers=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:56:00.062349Z","iopub.execute_input":"2024-08-30T07:56:00.062842Z","iopub.status.idle":"2024-08-30T07:56:03.677891Z","shell.execute_reply.started":"2024-08-30T07:56:00.062793Z","shell.execute_reply":"2024-08-30T07:56:03.676936Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 39 key-value pairs and 363 tensors from /kaggle/working/models/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Mistral Nemo Instruct 2407\nllama_model_loader: - kv   3:                            general.version str              = 2407\nllama_model_loader: - kv   4:                           general.finetune str              = Instruct\nllama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo\nllama_model_loader: - kv   6:                         general.size_label str              = 12B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                          general.languages arr[str,9]       = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 40\nllama_model_loader: - kv  10:                       llama.context_length u32              = 1024000\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  19:                          general.file_type u32              = 15\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 131072\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  22:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = tekken\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nException ignored on calling ctypes callback function: <function llama_log_callback at 0x7ab2dba1f760>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/llama_cpp/_logger.py\", line 30, in llama_log_callback\n    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 128: invalid continuation byte\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/Mistral-Nemo-Instruct-240...\nllama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 280\nllama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q4_K:  241 tensors\nllama_model_loader: - type q6_K:   41 tensors\nllm_load_vocab: special tokens cache size = 1000\nllm_load_vocab: token to piece cache size = 0.8498 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 131072\nllm_load_print_meta: n_merges         = 269443\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 1024000\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 1000000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 1024000\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 12.25 B\nllm_load_print_meta: model size       = 6.96 GiB (4.88 BPW) \nllm_load_print_meta: general.name     = Mistral Nemo Instruct 2407\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 1196 'Ä'\nllm_load_print_meta: max token length = 150\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n  Device 1: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.51 MiB\nllm_load_tensors: offloading 40 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 41/41 layers to GPU\nllm_load_tensors:        CPU buffer size =   360.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  3265.43 MiB\nllm_load_tensors:      CUDA1 buffer size =  3497.87 MiB\n.........................................................................................\nllama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: n_batch    = 32\nllama_new_context_with_model: n_ubatch   = 32\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =    42.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =    38.00 MiB\nllama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nllama_new_context_with_model:      CUDA0 compute buffer size =     8.50 MiB\nllama_new_context_with_model:      CUDA1 compute buffer size =    19.38 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\nllama_new_context_with_model: graph nodes  = 1286\nllama_new_context_with_model: graph splits = 3\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {%- endif %}\\n    {%- if message['role'] == 'user' %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- '[INST] ' + system_message + '\\\\n\\\\n' + message['content'] + '[/INST]' }}\\n        {%- else %}\\n            {{- '[INST] ' + message['content'] + '[/INST]' }}\\n        {%- endif %}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- ' ' + message['content'] + eos_token}}\\n    {%- else %}\\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n    {%- endif %}\\n{%- endfor %}\\n\", 'llama.embedding_length': '5120', 'llama.feed_forward_length': '14336', 'general.license': 'apache-2.0', 'llama.attention.value_length': '128', 'tokenizer.ggml.add_bos_token': 'true', 'general.size_label': '12B', 'general.type': 'model', 'general.version': '2407', 'quantize.imatrix.chunks_count': '128', 'llama.context_length': '1024000', 'general.name': 'Mistral Nemo Instruct 2407', 'tokenizer.ggml.bos_token_id': '1', 'general.basename': 'Mistral-Nemo', 'quantize.imatrix.entries_count': '280', 'llama.attention.head_count_kv': '8', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.attention.head_count': '32', 'llama.block_count': '40', 'llama.attention.key_length': '128', 'general.finetune': 'Instruct', 'general.file_type': '15', 'tokenizer.ggml.pre': 'tekken', 'llama.vocab_size': '131072', 'quantize.imatrix.file': '/models_out/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407.imatrix', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.last and system_message is defined %}\n            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n        {%- else %}\n            {{- '[INST] ' + message['content'] + '[/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + eos_token}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n\nUsing chat eos_token: </s>\nUsing chat bos_token: <s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Тестируем модель\n\nПроверяем, что модель действительно быстро отвечает и не делает просчет через CPU","metadata":{}},{"cell_type":"code","source":"\ntemplate = \"\"\"\nТы рускоязычная модель. Отвечаешь только на русском языке\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate.from_template(template)\nllm_chain = prompt | llm\nquestion = \"\"\"\nНа основании текста ниже ответь на вопрос\n\n###\nИзобрести гибридный натриево-ионный АКБ удалось исследователям Корейского института науки и технологий (KAIST). Ученые создали элемент питания, который способен заряжаться за несколько секунд, при этом аккумулятор обладает и большой емкостью, и увеличенной плотностью энергии относительно стандартных литий-ионных батарей. Особенность разработки в том, что натрий в тысячи раз имеет большее распространение на Земле, чем литий, поэтому производство АКБ на его основе будет и дешевле, и проще, и перспективнее.\nСообщается, что исследовательская группа объединила материалы анодов с катодами, подходящими для суперконденсаторов. Такой дуэт позволит получившемуся аккумулятору продемонстрировать сверхбыструю зарядку и разрядку с высокой емкостью самой батареи. Опыт же показал, что изобретение жизнеспособно и даже сможет стать альтернативой литий-ионным вариантам. По характеристикам плотности энергии разработка ничуть не уступает суперконденсаторам.\n###\n\nО чем говорится в тексте?\n\n\"\"\"\n# llm_chain.invoke({\"question\": question}, stop=stop_sequence)\n# output = llm.invoke(question)\noutput = llm_chain.invoke({\"question\": question})\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T07:56:15.683509Z","iopub.execute_input":"2024-08-30T07:56:15.684759Z","iopub.status.idle":"2024-08-30T07:56:26.540729Z","shell.execute_reply.started":"2024-08-30T07:56:15.684717Z","shell.execute_reply":"2024-08-30T07:56:26.539808Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\nllama_print_timings:        load time =     344.26 ms\nllama_print_timings:      sample time =     334.63 ms /   176 runs   (    1.90 ms per token,   525.95 tokens per second)\nllama_print_timings: prompt eval time =    4186.01 ms /   335 tokens (   12.50 ms per token,    80.03 tokens per second)\nllama_print_timings:        eval time =    6068.79 ms /   175 runs   (   34.68 ms per token,    28.84 tokens per second)\nllama_print_timings:       total time =   10821.35 ms /   510 tokens\n","output_type":"stream"},{"name":"stdout","text":" В тексте говорится о новом типе аккумуляторной батареи (АКБ), которая была изобретена исследователями Корейского института науки и технологий (KAIST). Эта новая АКБ является гибридным натриево-ионным вариантом, который обладает способностью заряжаться всего за несколько секунд, а также демонстрирует большие значения емкости самой батареи. По сравнению с обычными литий-ионными АКБ, изобретение является более дешевым и простым в производстве, поскольку натрий имеет гораздо большее распространение на Земле, чем литий. По своим характеристикам плотности энергии разработка ничуть не уступает суперконденсаторам.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}